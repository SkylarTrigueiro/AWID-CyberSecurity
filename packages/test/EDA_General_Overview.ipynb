{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This project is about detecting cyber attacks over a wifi network and how I developed a classifier with (relatively) high recall (0.83). In this dataset, wifi activity is classified as either normal, flooding, injection or impersonation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import HTML\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_model.processing.data_management import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig = load_dataset(file_name='AWID-CLS-R-Trn.csv')\n",
    "test_orig = load_dataset(file_name='AWID-CLS-R-Tst.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "With almost 1.8 million rows and 155 columns even the reduced data set is too large to explore each individual case. Exploring the meaning of the columns is more feasible but I will leave out most of that exploration.\n",
    "\n",
    "The data from this project is sourced from the AWID project (http://icsdweb.aegean.gr/awid/index.html). If you would like to use this data, please go to their website and ask for permission. The data is broken up into 4 different data sets, A larger data set (F) and a reduced version (R). For each dataset size, there is one that generalizes wifi activity into those mentioned earlier (CLS) and one that has more differentiation for each type of cyber attack (ATK). I will be focusing on the reduced dataset with more generalized classes for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1795574, 155)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(575642, 155)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal           90.956374\n",
       "injection         3.641120\n",
       "impersonation     2.702311\n",
       "flooding          2.700195\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*train_orig['class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal           92.207309\n",
       "impersonation     3.488105\n",
       "injection         2.897982\n",
       "flooding          1.406603\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*test_orig['class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "From the plot, we can see that a lot of the features almost entirely consist of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "missing_data = pd.DataFrame()\n",
    "missing_data['feature'] = train_orig.columns\n",
    "\n",
    "missing = list(100*train_orig.isnull().mean())\n",
    "missing_data['missing'] = missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 64))\n",
    "sns.barplot(data=missing_data, x = 'missing', y = 'feature' );\n",
    "plt.title('Missing Values');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names are difficult to interpret unless you are used to looking at wifi packet information. If you are interested, please look at https://www.wireshark.org/ for more information on these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "A lot of data preparation is about to take place behind the scenes. For a complete description of all of the behind the scenes you can check out my code under the processing section to see what's happening under the hood of these functions. I will do my best to summarize many of the changes here.\n",
    "\n",
    "Inside the prepare_data function I am replacing all missing values inside categorical variables with the label 'missing' as well as adjusting the time feature so that it is easier to measure. Later, you will see later that this data measures wifi traffic for an hour in the training set and approximately 20 minutes in the test set. I am also creating an integer feature that simply counts the seconds that have passed to use for aggregation. Why 1 second? Good question, I haven't yet done any analysis to determine if this is the best unit of time for aggregation, but it seemed like a good starting point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_ann_model.processing.data_management import prepare_data\n",
    "\n",
    "tt = pd.concat([train_orig, test_orig])\n",
    "\n",
    "X_train, y_train = prepare_data(train_orig, train_data=True)\n",
    "X_test, y_test = prepare_data(test_orig, train_data=False)\n",
    "X_train_test, y_train_test = prepare_data(tt, train_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train.copy()\n",
    "train['class'] = train_orig['class']\n",
    "\n",
    "test = X_test.copy()\n",
    "test['class'] = test_orig['class']\n",
    "\n",
    "train_test = X_train_test.copy()\n",
    "train_test['class'] = tt['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Target Distribution\n",
    "\n",
    "As you can see in the plots that follow there is a time dependency in the data for the target values flooding, injection, and impersonation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(content):\n",
    "    display(HTML(content))\n",
    "    \n",
    "def timehist(df, target, tcol, col, title = None, clipping=9999999999999999):\n",
    "        \n",
    "    df[df[target] == 'normal'].set_index(tcol)[col].clip(0, clipping).plot(style='.', figsize=(15, 5), label='normal')\n",
    "    df[df[target] == 'flooding'].set_index(tcol)[col].clip(0, clipping).plot(style='.', figsize=(15, 5), label='flooding')\n",
    "    df[df[target] == 'injection'].set_index(tcol)[col].clip(0, clipping).plot(style='.', figsize=(15, 5), label='injection')\n",
    "    df[df[target] == 'impersonation'].set_index(tcol)[col].clip(0, clipping).plot(style='.', figsize=(15, 5), label='impersonation')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timehist(df=train, target='class', tcol='frame.time_epoch', col='frame.time_delta', title='Training Data: Distribution of Traffic time deltas over time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timehist(df=test, target='class', tcol='frame.time_epoch', col='frame.time_delta', title='Test Data: Distribution of Traffic time deltas over time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timehist(df=train_test, target='class', tcol='frame.time_epoch', col='frame.time_delta', title='Train and Test Data: Distribution of Traffic time deltas over time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_model.processing.data_management import partition_features\n",
    "NUMERIC, CATEG = partition_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostic_plots(df, variable):\n",
    "    # function takes a dataframe (df) and\n",
    "    # the variable of interest as arguments\n",
    "\n",
    "    # define figure size\n",
    "    plt.figure(figsize=(16, 4))\n",
    "\n",
    "    # histogram\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.distplot(df[variable], bins=30)\n",
    "    plt.title('Histogram')\n",
    "\n",
    "    # Q-Q plot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
    "    plt.ylabel('Variable quantiles')\n",
    "\n",
    "    # boxplot\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.boxplot(y=df[variable])\n",
    "    plt.title('Boxplot')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Numeric variables ####\n",
    "for feat in NUMERIC:\n",
    "    print( 'Feature:',feat)\n",
    "    print('')\n",
    "    print(X_train[feat].describe())\n",
    "    print(' ')\n",
    "    diagnostic_plots(X_train, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for feat in CATEG:\n",
    "    print( 'Feature:',feat)\n",
    "    print('')\n",
    "    print('Number of unique values:')\n",
    "    print(X_train[feat].nunique())\n",
    "    print('')\n",
    "    print('Value distribution:')\n",
    "    print((X_train[feat].value_counts().head(20)))\n",
    "    print('')\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
