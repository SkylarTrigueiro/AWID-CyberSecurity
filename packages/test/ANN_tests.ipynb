{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_ann_model.processing.data_management import load_dataset\n",
    "train = load_dataset(file_name='AWID-CLS-R-Trn.csv')\n",
    "test = load_dataset(file_name='AWID-CLS-R-Tst.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_ann_model.processing.data_management import prepare_data\n",
    "\n",
    "X_train, y_train = prepare_data(train)\n",
    "X_test, y_test = prepare_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_ann_model.processing.data_management import load_pipeline_keras\n",
    "tf_ann_pipe = load_pipeline_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = tf_ann_pipe.predict(X_train)\n",
    "y_test_pred = tf_ann_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_train = np.argmax(y_train, axis=1)\n",
    "y_train = encoder.inverse_transform(y_train)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "y_test = encoder.inverse_transform(y_test)\n",
    "\n",
    "y_train_pred = encoder.inverse_transform(y_train_pred)\n",
    "y_test_pred = encoder.inverse_transform(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['flooding', 'impersonation', 'injection', 'normal']\n",
    "cm = confusion_matrix(y_train, y_train_pred, labels)\n",
    "plt.figure(figsize=(10,10))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['flooding', 'impersonation', 'injection', 'normal']\n",
    "cm = confusion_matrix(y_test, y_test_pred, labels)\n",
    "plt.figure(figsize=(10,10))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n",
    "ax.set_title('Confusion Matrix');\n",
    "ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "data_pipe = Pipeline(tf_ann_pipe.steps[:-2])\n",
    "data_pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_dataset(file_name='AWID-CLS-R-Tst.csv')\n",
    "test_ds = downsample(test)\n",
    "X_test, y_test = get_target(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bad = test_ds[y_test != y_test_pred]\n",
    "y_test_bad = y_test[y_test != y_test_pred]\n",
    "X_test_bad = data_pipe.transform(X_test_bad)\n",
    "X_test_bad['class'] = y_test_bad\n",
    "X_test_bad['frame.time_epoch'] = test_ds['frame.time_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import HTML\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_ann_model.processing.feat_eng_categ import one_hot_encoder\n",
    "ohe = one_hot_encoder(features='class')\n",
    "X_test_bad = ohe.fit_transform(X_test_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(content):\n",
    "    display(HTML(content))\n",
    "    \n",
    "def timehist(df, tcol, target, col, target_first, clipping=9999999999999999, concat_df = False, odf = None):\n",
    "    if concat_df == True:\n",
    "        df = pd.concat([df, odf])\n",
    "        \n",
    "    title = target + ' Hist ' + col\n",
    "    if( target_first==True):\n",
    "        df[df[target] == 1].set_index(tcol)[col].clip(0, clipping).plot(style='.', figsize=(15, 3))\n",
    "        df[df[target] == 0].set_index(tcol)[col].clip(0, clipping).plot(style='.', figsize=(15, 3))\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "    elif( target_first==False):\n",
    "        df[df[target] == 0].set_index(tcol)[col].clip(0, clipping).plot(style='.', title= title, figsize=(15, 3))\n",
    "        df[df[target] == 1].set_index(tcol)[col].clip(0, clipping).plot(style='.', title= title, figsize=(15, 3))\n",
    "        plt.title(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _desc(data, col, label):\n",
    "    d0 = data.describe().reset_index()\n",
    "    d0.columns = [col, label]\n",
    "    return d0.append({col:'unique values', label:data.unique().shape[0]}, ignore_index=True) \\\n",
    "             .append({col:'NaNs', label:data.isnull().sum()}, ignore_index=True) \\\n",
    "             .append({col:'NaNs share', label:np.round(data.isnull().sum() / data.shape[0], 4)}, ignore_index=True) \\\n",
    "\n",
    "def desc(df_train, col, target, include_test=False, df_test=None):\n",
    "    d0 = _desc(df_train[col], col, 'Train')\n",
    "    d1 = _desc(df_train.loc[df_train[target] == 1, col], col, 'Train normal')\n",
    "    d2 = _desc(df_train.loc[df_train[target] == 0, col], col, 'Train not normal')\n",
    "    if( include_test):\n",
    "        d3 = _desc(df_test[col], col, 'Test')\n",
    "        d4 = _desc(df_test.loc[df_test[target] == 1, col], col, 'Test normal')\n",
    "        d5 = _desc(df_test.loc[df_test[target] == 0, col], col, 'Test not normal')\n",
    "    if( include_test):\n",
    "        dd = d0.merge(d1).merge(d2).merge(d3).merge(d4).merge(d5)\n",
    "    else:\n",
    "        dd = d0.merge(d1).merge(d2)\n",
    "    display(dd)\n",
    "    \n",
    "    h('<b>Most popular values (NaN = -999):</b>')\n",
    "    N = 10\n",
    "    d0 = df_train[[target,col]].groupby(col)[target].agg(['size','mean','sum']).reset_index().sort_values('size', ascending=False).reset_index(drop=True)\n",
    "    d0 = d0.head(N)\n",
    "    d0 = d0.rename({'size':'Count in train (desc)','mean':'Mean target train','sum':'Sum target train'}, axis=1)\n",
    "    display(d0)\n",
    "    \n",
    "    if( include_test):\n",
    "        d1 = df_test[[target,col]].groupby(col)[target].agg(['size','mean','sum']).reset_index().sort_values('size', ascending=False).reset_index(drop=True)\n",
    "        d1 = d1.head(N)\n",
    "        d1 = d1.rename({'size':'Count in test (desc)','mean':'Mean target test','sum':'Sum target test'}, axis=1)\n",
    "        display(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist1(df,col):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.hist(df[col], bins=70);\n",
    "    plt.title('Train histogram: ' + col);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr1(df,col):\n",
    "    N = None #10000\n",
    "    num_vars = [f for f in df.columns if df[f].dtype != 'object']\n",
    "    trx = df.head(N) if N is not None else df.copy()\n",
    "    corrs = trx[num_vars].corrwith(trx[col]).reset_index().sort_values(0, ascending=False).reset_index(drop=True).rename({'index':'Column',0:'Correlation with ' + col}, axis=1)\n",
    "    h('<b>Most correlated values with ' + col + ':</b>')\n",
    "    trx = pd.concat([corrs.head(6), corrs.dropna().tail(5)])\n",
    "    def linkx(val):\n",
    "        return '<a href=\"#c_{}\">{}</a>'.format(val, val)\n",
    "    trx['Column'] = trx['Column'].apply(linkx)\n",
    "    h(trx.to_html(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric(df_input, tcol_input, target_input, col_input, target_first_input, df_test, include_test):\n",
    "    timehist(df=df_input, tcol=tcol_input, target=target_input, col=col_input, target_first=target_first_input, odf=df_test, concat_df=include_test)\n",
    "    hist1(df_input,col_input)\n",
    "    desc(df_input, col_input, target_input, include_test, df_test)\n",
    "    corr1(df_input,col_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical(df, col, target, df_test, include_test):\n",
    "    desc(df, col, target, include_test, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc(df, tcol, target, col, target_first, df_test, include_test):\n",
    "    if col not in ['isFraud','TransactionDT']:\n",
    "        h('<h3 id=\"c_' + col + '\">' + col + '</h3>' + '<a style=\"font-size:11px\" href=\"#home\">(Jump to top)</a>')\n",
    "        categorical(df, col, target, df_test, include_test) if df[col].dtype == 'object' else numeric(df, tcol, target, col, target_first, df_test, include_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(X_test_bad.columns)\n",
    "for x in ['frame.time_epoch', 'class_normal', 'class_injection', 'class_impersonation', 'class_flooding']:\n",
    "    columns.remove(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in columns:\n",
    "    proc(df=X_test_bad, tcol='frame.time_epoch', target='class_normal', col=col, target_first=True, df_test=None, include_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_test_bad.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tf_ann_model.config import config\n",
    "from tf_ann_model.processing.data_management import load_dataset, get_target\n",
    "from tf_ann_model import pipeline\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(file_name=config.TRAINING_DATA_FILE)\n",
    "val = load_dataset(file_name=config.TESTING_DATA_FILE)\n",
    "\n",
    "X_train, y_train = get_target(data)\n",
    "X_val, y_val = get_target(val)\n",
    "    \n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_val = encoder.transform(y_val)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical( y_train, num_classes=4)\n",
    "y_val = tf.keras.utils.to_categorical( y_val, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fe_pipe.fit(X_train)\n",
    "X_val = pipeline.fe_pipe.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
